# ITKST53
Säilö ITKST53-kurssin palautuksille
## Luento 1
Turvallisuus määriteltiin seuraavasti: jonkin tehtävän tai tavoitteen saavuttaminen huolimatta mahdollisista uhkatekijöistä. Mielesäni uhka voi olla muutakin kuin hyökkääjä, mutta tällä kurssilla keskitytään lähinnä ns "bad actor" tilanteeseen. Luennolla esiteltiin tapa ajatella turvallisuutta seuraavista näkökulmista:

* **policy:** tavoite, tarkoitus, toiminnan määrittely. Näitä voi olla vaikka tiedon luotettavuus tai pääsyoikeuksien määrääminen. Policy määrää mitä mekanismi pyrkii saamaan aikaan, eli mekanismi siis toteuttaa policyä. Hyökkääjä voi pyrkiä hyväksikäyttämään tai rikkomaan tätä policyä, jota yritetään mallintaa uhkamallin avulla.
* **uhkamalli:** malli tai kuvitelma mitä uhka/hyökkääjä voi tehdä tai pyrkii tekemään. Vaikea luoda täydellistä uhkamallia, etenkin jos järjestelmä ei toimi tyhjiössä, kuten näytettiin esimerkkinä luonnolla salasanan nollaamisesta ja ukloisten järjestelmien hyväksikäytöstä. Uhkamalli ovat lähinnä olettamuksia.
* **mekanismi:** järjestelmä tai ohjelmisto, jolla valittu policy taataan. Mekanismeja voivat olla pääsynhallinta, salaus, tilit ja niiden käyttöoikeuksien erottaminen. Mekanismeissa voi olla aukkoja, joita ei huomioitu uhkamallissa. Mekanismin ei välttämättä tarvitse olla vedenpitävä, kunhan voidaan väsyttää hyökkääjä. Ei taida toimia hyvin varustautunutta/valtion tukemaan hyökkääjää vastaan. Yleisesti on hyvä pyrkiä käyttämään mekanismeja pienentämään tai kuristamaan hyökkäyspinta-alaa.

Vaikeus syntyy uhkamallien laajuudesta ja voidaanko ennustaa tai arvioida hyökkääjän käyttäytymistä? Policy voi sallia hyökkääjän toimia, joten sitä suunnitellessa pitäisi hahmottaa tietoturvallinen malli. Esimerksi salasanajärjestelmässä jäähyjen käyttä arvausyrityksiä vastaan on hyvä keino parantaa turvallisuutta laajamittaista useiden tilien murtautumista vastaan. Tosin tästä voi olla vähemmän hyötyä, jos hyökkäys on kohdistettu. Muiden järjestelmien kanssa toimittaessa pitäisi omat systeemit/politiikat kehittää siten, että kriittisessä toiminnassa, kuten tilinhallinassa ei olla riippuvaisia muiden järjestelmien paljastamista tiedoista. (esim. jos tilin palautukseen riittää tieto, joka on saatavilla muista järjestelmistä, voi politiikan uudelleen miettiminen olla paikallaan. Luentoesimerkeissä ei ilmeisesti aina kyetty asettamaan hyökkääjän asemaan).

Hyviä esimerkkejä uhkamallin ongelmista olivat esitellyt heikkoudet salauksen vahvuudessa/ sertifikaattien luotettavuudessa jotka yhdistyivät nätisti mekanismeihin. Käyttäjän toimet osana uhkamallia ovat myös kiinnostava näkökulma. Kuinka järjestelmän toiminta pitäisi suunnitella, jos käyttäjä voidaan huijata toimimaan sitä vastaan? Nähtävsti varmin tapa on määritellä oikeudet siten, että käyttäjä ei kykene yksin sabotoimaan järjestlemää. Uhkamallien pitäisi myös huomioida, että hyökkääjä hallitsee laitteen, jolla palvelun kanssa kommunkoidaan. Eli uhkamalli on suunniteltava siten, että palvelupyynnöt eivät ole ain oikein muotoiltuja. Uhkamalli jossa hyökkääj toimii käyttäjän välineillä ei ole realistinen.

Mekanismien ongelmat esiintyvät virhetilanteina tai bugeina. Luentoesimerkki applen icloud salananan arvausrajapinnasta, joka ei toteuttanut muiden rajapintojen tavoin salananan arvaukseen liittyvää policyä, tai citigroup palvelu, jossa ei tarkistettu sessiota ovat esimerkkejä räikeistä ja helposti ymmärrettävistä virheistä.

Mielestäni tämä merkitsee, että usein ei voida rakentaa täysin vedenpitävää järjestelmää. Hyökkääjän käyttämät menetelmät eivät aina vastaan oletusta ja käytetyillä työkaluilla(tarkistus, auditointi, ohjelmointikieli missä ei voida suoraan käsitellä muistiosoitteita/pinoa) voidaan karsia joitain riskitekijöitä. Kuitenkin pelikenttä on silti usein liian laaja ja nojautuminen yhteen mekanismiin on huono idea. Tekninen velka ja kolmannen osapuolen järjestelmät jo itsessään kehottavat rajoittamaan yhden järjestelmän murtumisen vaikutusta(mekanismit tässä: service accounts, eri kerrokset, järjestelmän eristäminen). Tällä kerralla hieman avattu ideaa, jonka mukaan ohjelmointikielen hallitseminen itsessään ei riitä, vaan on tunnettava sen erityspiirteitä. Keinoja näiden varalle voivat olla paremmat työkalut, parempi tietotaito tai kattavempi testaus. Työkaluilla tarkoitan siis kääntäjän tai kehitysympäristön tarjoamia varoituksia tunnetuista epäturvallista tavoista. Tietotaidon parantaminen on itsestäänselvyys mutta käytännössä hyökkääjien mekanismit ovat niin syvällä, että vähemmän kokenut ohjelmoija tulee tekemään virheitä. Sen lisäksi esimerkit ja (vanhentuneet?) oppaat saattavat ohjata aloittelijan väärälle tielle, josta palaaminen on vaikeampaa. 

## Luento 2
Ylivuodot (c/c++) ovat ohjelmointikielen ominaisuuksia, jossa muistiosoitteet ovat ohjelmoijan käsiteltävissä (huonon koodin kautta täten myös hyökkääjän). Perusesimerkki ylivuodosta on bufferin, tai array:n täyttäminen yli sille varatun rajan. Hyväksikäyttö hyödyntää x86-arkkitehtuurin tuntemusta ja laitteistopinon manipulointia. Aikasemmassa luennossa(luentoäiväkirjamerkinnässä) mainittiin ylivuotoja vastaan suojautuminen käyttämällä ohjelmointikieltä, joka ei palajasta raajoja muistiosoitteita. Tämä helpottaa ohjelmoijan työtä, mutta samalla siirtää vastuun kielen runtime-ympäristön päälle ja jos tarvitaan kirjastoja, jotka eivät käytä mem-safe kieltä, voidaan vain luottaa että ylivuotoja ei voi tapahtuma kolmannen osapuolen koodissa.
 
Ylivuotoja varten päätin kerrata(ja usein syventyä uuteen asiaan) miten ylivuodot tapahtuvat ja mitä mekanismeja siinä käytetään. Pino ja sen kehykset kyllä muistuivat mutta (myöhemmille luennoilla käytävät) rekisterit ja järjestelmäkutsut vaativat pidemmän hetken. Tämä oli tärkeä sillä os labran tehtävistä (joita en siis palauta kurssin suoritusta varten mutta teen ja katselen omaan tahtiin, riippuen ajasta ja mielenkiinnosta) vaativat syvällistä tietoa rekistereistä. Aikamoinen hyppäys, jos eniten ohjelmointikokemusta on muistiturvallisista kielistä :-/. 
 
Ylivuotojen välttämiseen ehdotettiin bugien välttämistä. Erinomainen ehdotus, onnistunee mahdollisesti pienempien sovelluksien kanssa mutta kuten luennolla 1 mainittiin, niin nykyiset järjestelmät ovat entistä monimutkaisiempia. Ja kuten on tullut esille, hyökkääjän hyväksikäyttämät bugit ovat syvällä ohjelmointikielen yksityiskohdissa. Tämä on jo yksi syy, miksi bugiton ohjelmointi on haave, kaunis uni. Myös materiaali, jonka pohjalta on kukin opiskellut voi osaltaan vaikuttaa(vaikeuttaa) bugien välttämistä. Toinen tapa (pyrkiä) estämään ylivuotoja mahdollistavat bugit on analysoida ohjelmallisesti kirjoitettu lähdekoodi ja käyttää saatuaj tietoja testauksen alustamiseen tai ohjelmoidan varoittamiseen. Hyvä muistutus on pyrkiä ulottamaan testaus ohjelman suorituspuun jokaiselle "oksalle". Viimeinen esitelty tapa on käyttää ohjelmointikiletä ja ympäristö missä ylivuotoja ei tapahtudu, koska kirjoitettuohjelma ei käsittele suoraan muistia tai sen osoittimia.
 
Baggy bounds oli uusi asia ja tuotti kyllä hieman vaikeuksia hahmottaa sen toimintaperiaatetta kokonaisuudessaan julkaisua lukemalla (kuten luennoitisija mainitisi). Perusperiaate on varata muistia 2^x lohkoissa ja osoitin operaation kohdalla tarksitetaan onko osoitin sille varatulla alueella. Tämä varattu alue tod. on isompi, kuin itse objekti. Baggy bounds rajojen ulkopuolelle jäävät osoittimet merkitään siten, että niitä ei käsitellä. Tämän metodin ongelma on, että se vaatii kääntäjän tuen. Uskoakseni NX-mem ja erinäiset muistiavaruuden hajautusmenetelmät ovat yleisempiä. Syy on todennäköisesti, että vaikka baggy bounds aiheuttaa muihin vastaviin menetelmiin nähden vähemmän hävikkiä, on kehittäjille silti helpompi käyttää muita menetelmiä (kuten: DEP, aslr).  Luennolla 3 käytiin sen periaatetta tarkemmin läpi, joten säästän sinne tarkemman kirjoittelun. Tosin tässä on huomattavaa, että missä NX-muisti (non-exec) suojaa injektoidulta, hyökkääjä voi silti hyödyntää ohjelman muistissa sijaitsevia (tai ladattujen kirjastojen muistissa) olevia palasia (myöhemillä kerroilla kutsuttiin gadget:ksi, suomeksi vehje?), eli ROP.
 
Kuitenkin canaries periaate on looginen ja aikaisemmin tuttu wiki-artikkelien pohjalta. Myöskin sen ongelmat, eli jos canaryn arvo on staattinen tai ennaltaarvattava (kiinnostava idea oli käyttää jotain signaalia esimerksi istunnon tai socketin sulkemista signaalina staattisen canaryn etsimisessä. Myös pinon tilasta vuotavat menetelmät olivat uusia tuttavuuksia minulle, joku string-format metodi. Tarvittiin tarkempi tarkastelu OWASP-wikissä idean selvittämiseksi. Muistan lapsuudesta erään verkkopelin, jossa serveri saatiin kaadettua kirjoittamalla yleiseen chattiin %n%n%n%n%n%n, joka taitaa liittyä juurin tähän string format menetelmään). Muita keinoja, joissa tallennetaan muistin varauksen yhteydessä varatun muistin koko erilliseen tietueeseen on esitetty mutta niillä on oma suorituskyky/muistin tarpeensa (tarpeeksi iso, jotta tekniikkaa ei voida käyttää tuotannossa tai kriittisissä sovelluksissa).

## Luento 3

Luennolla kolme jatkettiin baggy bounds mekanismin kanssa ja tarkasteltiin erinäisiä ylivuoto suojauksia ja niitä vastaan kehitettyjä tekniikoita. Hyökkäystekniikoista oli tarkemmin käsittelyssä return oriented programming:n sokea muunnos, jossa rop-metodeja sovellettiin etäkohteeseen, hyödyntämällä kohdepalvelun toimintaa signaalina. Signaali voi olla yhteyden sulkeutuminen (palvelin kaatui) tai pysäytys (stop-vetkuttimen etsimisessä). Vartijalohko(canary):n kukistamiseen ehdotettiin ympäristössä, missä canary on 8-tavuinen, raanvoiman käyttöä arvauksessa, jos voidaan tunnistaa väärä bitin arvon arvaus jollain mekanismille (kuten kaatuminen). Oletuksena siis että palvelu käynnistetään kaatumisen jälkeen uudestaan ja sen prosessin osoiteavaruus, sekä canary ei muutu näiden käynnistyksien välillä (huom! fork() vs execv(), missä fork()). Baggy bounds systeemi, etenkin 64-bittisessä ympäristössä, missä osoittimen rajat sisällytetään siihen itseensä, ylempiin bitteihin. Etuina mm. osoittimen välitys BB ja non-BB koodin välillä. Tosin luennolla mainittu 64-bittisen osoittimen rakenna ja selitys, että tietyt bitit voivat olla pakosta 0 jäi hieman mietityttämään. Ainakin BROP-paperi linjaa normaaleilla prosesseilla ensimmäiset 2 tavua nolliksi. Muut hyökkäyskeinot kuten funktion osoittimen ylikirjoitus, kiertävät canaryn koska hyökkääjän ei tarvitse ylittää canaryä muokatakseen ret.addr:ää. BROP-paperi tosin mainitsi, että bufferi (joka ylivuotaa) yleensä sijoitetaan pinossa siten, että ylivuoto ei osu muihin muuttujiin ennen canaryä.

Hyökkääjän koodin injektion estoon tarkoitettu NX-stack voidaan kiertää hyödyntämällä olemassa olevaa koodia(palasia, kuten tiettyjä operaatiokoodeja, pop,$eax; ret yms.). Hyökkääjä muokkaa pinoa siten, että haluttu sekvenssi näitä olemassa olevia opkoodeja suoritetaan. Muutenkin keinot, jotka ovat automaattisesti (tai oletuksena) päällä ja eivät aiheuta vääriä hälyytyksiä ovat yleisempiä. Mainittiin myös, että on todennäköistä että ohjelma tukee osoiteavaruuden satunnaissekoitusta (poislukien tapaukset missä tarvitaan tiettyä muistiosoitetta mm. ajurit tai muut lähellä rautaa toimivat komponetit). 

Pop; ret; yms vekottimet tuppaavat aiheuttamaan hieman harmaita hiuksia. Niiden käyttökohde on kuitenkin (BROP-paperissa) pinon saattaminen tilaan, jolla voidaan luoda (64-bittisessä arkkitehtuurissa) järjestelmäkutsu valituilla argumenteilla. Monissa tapauksissa itse hyökkäys käyttää useita tekniikoita hyväksi. Kun threat model BROP:ssa ja muissa hyökkäyksissä missä hyökkääjä useilla arvauksilla kiertää satunnais- yms muut suojaukset, että palvelimelle voi lähettää useita arvauksia, tai muuten tökkiä sitä ja reaktiosta päätellään jotain. Ilmeinen vastatoimi olisi siis uudelleensekoitus uuden lapsiprosessin luomisen yhteydessä, kenties poikkeavan toiminnan tunnistaminen jollain menetelmällä voisi toimia? Itseasiassa yllätyin, kuinka monipuolisesti ilmaisia työkaluja tämän mallisien hyökkäyksien ja payloadien rakentamiseen on yleisessä jakelussa olevassa metasploit-ympäristössä. Esimerksi msfconsole:lla voidaan generoida hyökkäyskoodi, sekä määritellä kielletyt tavut (esimerkiksi \0, jos ylivuoto tapahtuu merkkijonoa käsitellevässä komponetissa missä \0 tavu lopettaa tavujen kirjoittamisen muistiin).

Yhteenvetona kolmesta ensimmäisesti luennosta voisin vetää, että esitellyt mekanismit (BROP, canary, yms) tekevät joitain olettamuksia palvelimen toimista. Toisin sanoen hyökkäyksille on määritelty threat model. Esitellyt menetelmät voidaan kukistaa elimoimalla ylivuodot, bugittomalla ohjelmoinnilla, rajojen tarkastuksella, tai muilla keinoilla. Koska hyökkäyksillä uhkamallissa tehtiin muutama oletus, voidaan ne hyökkäykset estää tekemällä muutoksia kohdejärjestelmään (osoitteistuksen uudelleen hajautus, ei käytetä fork():a joka perii vanhemmalta prosessilta yms). Tosin heap-ruiskutus voi toimia myös näissä tapauksissa, monimutkaisia juttuja.. Myös entropian määrä on mietittävä asia, esimerkkinä annettiin hash-funktiolle annettava seed, joka on tällä kertaa timestamp, joka vähentää tuntemattomien bittien määrää reilusti. On myös myönnettävä, että ennen luentoja tunsin vain normaalin ylivuodon, jossa shellcode injektoidaan sekä hypyn johonkin olemassa olevaan osoitteeseen. Vekottimet (gadget) tulivat siis uutena asiana ja hyödynsivät konsepteja, joita ylemmän abstraktiotason kanssa työnskentelevät eivät aina ajattele. <del>Jos ei mitään muuta ota päähän, niin edes se vanha totuus että seuraavia metodeja ei kannata koskaan käyttää (oman puuhastelun ulkopuolella, eikä silloinkaan kannata, ettei jää paha tapa).</del> Päivitys: taitaa olla järkevämpi jättää pahojen metodien tarkistus ja kirjanpito ohjelmiston hoidettavaksi käyttämällä vaikka banned.h tai vastaavaa keinoa varoittamaan, jos on sattunut kömmähdys.

Sen lisäksi voidaan hyödyntää staattista tarkistusta tai muita työkaluja etsimään mahdollisia ylivuodon kohteita, tai käyttämään syötteenä erinäisille rajojen tarkistus menetelmille. Toisin sanoen siis koska rajojen tarkistus aiheuttaa suorituskyky hävikkiä, kohdennetaan tarkistus tunnettuihin komponetteihin ja jätetään tarkistukset pois niiltä koodipoluilta, joiden osalta voidaan olla varmuja, ettei hyökkääjä pääse niistä hyötymään. Ehkä helpointa on arvioida, jos sovellus, joka on käyttää käyttäjän syöttämää dataa (tätähän voitaisiin suodattaa tai kkäsitellä, sen sijaan että siihen luotetaan..) voidaan ohjelmoida käyttämällä muistiturvallisia kieliä ja välttää c-kielisiä kirjastoja. Noh, aina saa unelmoida.

## Luento 4

Oikeuksien hallinta järjestelmänäkökulmasta, on idea missä järjestelmän komponenttien oikeuksia tai mahdollisuuksia rajoitetaan mahdollisimman paljoan, estämättä toiminnallisuutta. Aikaisemmin on tullut esille, että jos hyökkääjä voi suorittaa omaa koodia tai hyödyntää olemassa olevia vekottimia, niin nämä toiminnon suoritetaan haavoittuneen prosessin oikeuksilla. Unix:ssa, johon liittyy tuleva asia, proesssilla on yksi omistaja ja mahdollisesti useita ryhmiä, missä inode:lla (tiedsotot ja hakemistot) on vain yksi omistaja ja ryhmä. Oikeuksien vähentäminen ja prosessien ajaminen mahdollisimman vähillä oikeuksilla on erityisen tärkeää, jos kyseinen prosessi tekee jotain käyttäjältä tulevan syötteen kanssa. Ohjelmointivirheet, tai (oudot ja ihmeelliset) policy:t (case: https://www.evilsocket.net/2017/05/30/Terramaster-NAS-Unauthenticated-RCE-as-root/) missä web-hallintapaneelin (php) kautta voidaan lähettää tiedostoja (mihin tahansa sijaintiin), kunhan cookie[kod_name] on olemassa (mikä tahansa arvo käy). Esimerkin ongelmat, jotka liittyvät tämän kertaisen luennon aiheisiin: 
* tiedostoja voitiin lähettää mihin tahansa sijaintiin tiedsotojärjestelmässä. Mahdollistaa omien .php tiedostojen suorittamisen lisäämällä ne /usr/www/ hakemistoon ja kutsumalla niitä. Tässä olisi voitu rajoittaa vastaanottaminen omaan hakemistoonsa käyttämällä chroot:ia.
* web-palvelin on käynnistetty root:na, jolloin voidaan suorittaa haluttuja komentoja pääkäyttäjän oikeuksilla käyttämllä esimerksi php system() komennolla.

Merkittävin ongelma on tietenkin kelvoton käyttäjän autentikointi, mutta sopivalla menettelyllä oltaisiin voitu hidastaa hyökkääjää.  

Unix:in oikeuksien hallinta kuvattiin kohteen (subject), objektin (file, file descriptors, prosessit ja minusta yksi objekti, joa jäi välistä olisi käyttäjätilit). Prossessilla on yleensä yksi käyttäjätunnus (uid) ja useampi ryhmätunnus, jotka määrittelevät prosessin oikeudet. Unix tiedostoilla/hakemistoilla on omistaja ja ryhmä(uid ja gid) ja permission(matriisi/table, rwx-maski miten sen nyt ottaa). Omistajalla on ainoat oikeudet muuttaa oikeuksia. File descriptor on tavallaan kahva johonkin tiedostoon, joka voidaan ohjata muille. Tällä tavalla, kuten luennoitsija kertoi, voidaan rajoittaa prosessin pääsyä tiedostoihin. Yksi teema on nähtävissä jo alkuluennosta, eli eristäminen, oli kyse sitte nteidostoista tai prosesseista tai niiden muistista. Verkkojen kanssa voidaan avata yhteyksiä (periaatteessa) rajatta. Alhaisien <1024 porttien kuuntelu on rajoitettu tietyille käyttäjille (missä super user / root kävelee tarkistuksien ohi). Käyttäjätunnukset (uid) voidaan määrittää prosessille järjestelmläkäskyllä. Chroot -komento toimii tavallaan komerona, eli komento tavallaan muokkaa miten /-käsitellään. Missä / osoittaa takasin itseensä, jolloin juuren / käsite muuttuu tälle prosessille. 

OKWS-kehys pyrkii jakamaa tyypillisen palvelimen komponentit useiksi käyttäjiksi, tarkoituksena eristää www-palvelin tietokannasta tiedostojärjestelmän näkökulmasta. Koska jos palvelimen kaikki komponentit suoritetaan samalla omistaja käyttäjällä, yhden komponentit kautta hyökkääjä saa nämä oikeudet myös muiden sillä suoritettujen proesssien tietoihin. OKWS:ssä pyyntö käsitellään okd-prosessin taholta, joka ohjaa pyynnön lokikompontentille, tai palvelulle. Palvelut käyttävät tietoakntaa välittäjäkomponentin kautta. Ideana on eristää tietokanta ja rajoittaa sen suoraa käyttöä vain tietylle proessille. Mainittiin, että OKWS:ssä jokaisella prosessilla ja palvelulla on oma käyttäjätunnus uid.

Muuteknin oikeuksien rajaaminen ja eristäminen on mielekästä ja sinänsä turvallisempaa, jos onnistunut hyökkäys vaatii useden käyttäjien kaappauksen. Kenties tästä kaappauksesta tai sen yrityksestä jää tarkemmat lokitiedostot jälkiselvitystä varten, esimerkiksi soveltamalla videon opiskelijanan ideaa missä lokitiedostoon kirjoittava komponetti voi vain lisätä rivejä lokitiedostoon hyödyntämällä unix:in oikeuksienhallintaa. Sinänsä groupid tuo mieleen active directoryn, jossa on myös käyttäjäkohtaisia ja ryhmäkohtaisia rajoituksia, oikeuksia ja rajoituksia. Kuten on aiakisemmin tullut esille, tässä on pääasiassa nähtävästi tarkoituksena asettaa mahdollisimman monta estettä hyökkääjälle ja rajoittaa/pitää mahdollinen vahinko kontrollissa. Monet OKWS-järjestelmän ratkaisut tähtäävät tähän toiminnallisuuden eritämiseen, esimerkiksi staattinen ja dynaaminen sisältökin on erotettu, pubd toimii vain templaattien jakelijana. Palvelimelle määritellyt "periaatteet" principles (outo termi tähän yhteyteen, vaikeahko ymmärtää sen tarkoitusta), siis pyrkivät suojaamaan muita komponentteja ja tiedostoja/tietoja ohjelmoointivirheiden seurauksilta. Tähän viitataan OKWS-paperissa kappaleessa 2.2, missä mietitään voiko apache, php, olla virheettömiä ja ratkaisevat näin ongelmat. 

## Luento 6

Oikeuksiin liittyy useita ongelmia, jossa yhdessä prosessilla voi olla kaksi erilaista oikeutta ja vaikeudet syntyvät kumpaa oikeutta pitäisi käyttää. Prosessilla oi olla laajemmat oikeudet, kuin sitä ohjastavalla käyttäjällä, jolloin käyttävä voi esimerksi avata prosessiin tiedostoja, jotka eivät ole käyttäjän normaalien oikeuksien rajoissa. Ambient authority, varmaan suomentamalla leijuvat oikeudet, määritellään oikeuksiksi, joita prosessi käyttää automaattisesti. Tämä voi esiintyä vaikka oikeutena avata tietty tiedosto jostain hakemistosta. Tämä luo ongelman, jossa on haastavaa hallita tarkasti miten prosessi voi käyttäytyä, jos sillä on jotain oikeuksia joita käytetään tarvittaessa. Yksi ratkaisua jota voidaan ajatella on, manittu fd:den käyttäminen tiedostonimien sijasta. Tällöin prosessi jolla on  käytössään jokin fd, on myös todennettavasti ollut oikeudet avata se (fd:n saaminen käyttöön, itse avaamalla tai muulta prosessilta, tarkoittaa että avaavan prosessin oikeudet siihen tarksitetaan avauksen yhteydessä).

Capability määritellään: väärentämätön osoitus jostain oikeudesta tai valtuutuksesta. Luennoilla käsiteltiin lähinnä jaettuna file descriptionina, missä suorittavalla komponetilla ei ole yhtään ylimääräisiä oikeuksia ja tarvittavien tiedostojen aukaiseminen suoritetaan siihen tarkoitetulla kompontentilla. Nähtävästi asettaa haasteita muun muassa kääntäjille. 

Virtualisoinnin käyttäminen eristykseen, kun moista tarvitaan, mutta ei sovellu (helposti) tapauksiin missä pitää antaa oikeuksia tiettyihin tiesotoihin, tai jakaa tietoa useiden vm:ien välillä. Verkkokerroksen eristys voi olla hankalampaa. Sockettien kuuntelu on yksi toimenpide missä tarvitaan laajempia oikeuksia. 

Global namaspaces: tiedostojärjestelmä -> vaikea kontrolloida oikeuksia, prosessin uid ja objektien omat oikeudet/asetukset, voi olla rajaton, capsicum tarkoitus on pyrkiä kohti "file description" -mallia, missä globalia namespaceä ei ole käytössä. 

Capsicum on framework, jonka tarkoitus on vähentää epäluotettavan koodin oikeuksia ja näin pienentää hyökkäyspinta-alaa. Rajoittamalla oikeuksia voidaan kitkeä tapauksia missä kaapattu prosessi vaikuttaa muihin saman käyttäjän käynnistämiin prosesseihin. Käytännössä siis epäilyttävän (ulkopuolisen, syötettä käsitteleven tai monimutkaisen ja täten mahdollisesti bugisen) koodin suorittaminen tapahtuu ersitetyssä hiekkalaatikossa. Capsicum vaatii muutoksia ohjelmien toteutukseen mutta tarjoaa niihn valmiin kehyksen. Se laajentaa fd:tä sisältämään niin sanotun capabilityn, jolla voidaan määritellä tarkemmin sallitut operaatiota tietylle fd:lle. Capsicum toimii muutamassa vaiheessa:
* määritellään ja annetaan oikeudet
* siirrytään rajoitettuun tilaan kutsulla cap_enter

Rajoitetussa tilassa kyseinen prosessi (ja sen jälkeläiset) eivät voi tehdä muutoksia tai niille ei voida antaa uusia oikeuksia. Eli ennen tähän tilaan siirtymistä pitää avata ja lähettää tarvittavat fd:t. Capsicum voidaan toteuttaa joko kirjastolla, tai ytimen tasolla (cap_enter). Kirjasto ei suojaa, jos ohjelmoija ei huomaa käyttää sitä tehokkaasti tai jos suoritetaan kolmannen osapuolen koodia. Fd lists vaati hieman pohdintaa, videolla se selitettiin abstraktioksi, jossa fd (joka on numero) esitetään paremmin ymmärrettävänä nimenä, joka on ilmeisesti vielä hierarkkinen. Kenties se voi olla tutumpaa muotoa "fd.int : dir.dir.file.string"? Toinen seikka mikä nostettiin esille on kuinka käsitellään usea prosessille syötetty fd. Miten siis erotetaan ne toisistaan, käytetäänkö tiettyä järjestystä niin lähettämisessä (vrt. 0 - input, 1 - output, 2 - error.out), tai listoja tai array:tä?

Koska capsicum tekee muutoksia ytimeen, joilla saadaan hienojakoisempaa kontrollia, voidaan operaatioiden kohdalla määritellä tarkemmin mitä oikeuksia tai mahdollisia toimintoja sillä on. Missä unix primitiivit ovat luku, kirjoitus ja suoritus, capsicumin kohdalla voidaan määritellä useita oikeuksia maskeilla. Esimerkiksi voidaan sallia kirjoitus mutta estää tiedostossa liikkuminen. Tällöin kirjotius on mahdollista vain tiedostossa tiettyyn kohtaan. Tämä konsepti oli aluksi hankala käsittää sen esittävästä julkaisusta, videoluento auttoi tässä huomattavasti eteenpäin. Mielestäni tässä on ajatuksena siirtyä ohjelmien kohdalla kohti usean eristetyn proessin mallia, yhden massiivisen prosesin sijasta. Kun edellinen kohta liitetään tarkempaan oikeuksien hallintaa voidaan rajoittaa ensimmäisien luentojen hyökkäyksiä. Hyvä pointi tuli esille, missä jos capsicumissa käytetään fd:tä esittämään verkkoresurssia ja jostain syystä yhteys katkeaa, miten tästä selvitään eteenpäin? Pitäisi avata uusi fd tähän resurssiin ja palata uudestaan hiekkalaatikko-tilaan. Kuullostaa hankaalta käytännössä ja mahdollisesti aiheuttaa palvelun käyttökatkoja.

## Luento 17

Luettavassa paperissa manittiinkin jo alussa yksi syy miksi salasanojen turvallisuus on heikolla tasolla. Riippuen näkökulmasta, järjestelmää voidaan arvioida kolmesta näkökulmasta: 
* teknisesta tietoturvasta: kuinka vahva järjestelmä/menetelmä on murtaa.
* käytettävyys: miten järjestelmää voidaan käyttää ilman että se vaikeuttaa tai estää tehtävää toimintoa. Kokemuksieni mukaan käyttäjä ei arvosta turvallisuutta yhtä paljon kuin käytön helpoutta. Hän saattaa etsiä oikotietä tai käyttää lyhyttä salasanaa joka on helppo muistaa (ja yleensä myös arvata).
* menetelmän käyttöönotosta ja yhteensopivuudesta nykyisien järjestelmien kanssa.

Paperin tuloksien mukaan yksikään tunnettu järjestelmä ei ole optimaallinen jokaisesta näkökulmasta. Salasanaan tai tarkemmin sanottuna johonkin salaiseen merkkijonoon liittyvät järjestelmät ovat haavoittuvia (kenties, ainakin hyökkäyspinta-alaa on tarjolla) arvauksen, nuuskinnan, käyttäjän ja palautusmekanismin osalta. Myös palvelun määrittelemä salasanapolitiikka voi (hyvistä aikeista huolimatta) heikentää turvallisuutta, jos monimutkaiset säännöt ajavat käyttäjät valitsemaan helposti arvattavan salasanan/vähentävät salasanan satunnuisuutta helpottaen arvausta. Itse inhoan kuristavia salasanasääntöjä ja haluaisin nähdä liikettä salasanoista kohti salalauseita. Nykyään myös rate-limitng tai arvauksien yrittämisen estäminen ovat tärkeitä tekijöitä. Rajaamalla arvauksien määrää tai nopeutta, voidaan lisätä hyökkäyksen hintaa. Tosin luennoilla ei selvinnyt kannattaako arvauksia rajoittaa palvelu-, yhteys- vai/ja tunnuskohtaisesti. Myös ratkaisut missä käytetään jotain rautapohjaista generaattoria tai autentikaattoria (kuten eräs verkon mittaussovellus käytti usb pohjaista varmenninta) tai älypuhelin sovellusta, steam guard ja mobile autenticator ynnä muut vastaavat. Moniulotteiset mekanismit (varmaan väärä termi mutta se on helppo ymmärtää) käytetään tunnistautumisessa hyväksi jotain mitä käyttäjä tietää, omistaa (ja on, jos käytetään esim. biometeriikkaa). 2FA-mekanismit perustuvat juuri salasanan ja varmennuksen (toinen email, tai palveluun rekisteröitu puhelinnumero yms). 

Federated-järjestelmät pelottavat aina hieman. Jos kaikki on yhden kirjautumisen takana, niin murtautumisen vahingot ovat suuret. Itse olen käytännössä jumiutunut salasanoihin, mikä varmasti kielii jotain artikkelin sanomasta. Luennoilla muistutettiin myös salaisuuksien (joita käytetään tunnistautumisessa) tallennuksesta tietokantaa ja kuinka ne tallennetaan tavalla jolla on kallista selvittää mikä on alkuperäinen salaisuus (hash, salt). Kenties hyvä järjestelmä voisi olla käyttää hidasta hash-funktiota suolattuna, arvauksia rajoitettuna (yritetään välttää DoS) ja kenties vielä skannataan ja estetään yleisempien tunnettujen salasanojen käyttö. Käyttäjän salasana ja tunnus luonnollisesti läheteään palvelimelle salatun yhteyden kautta, jotta nuuskijat eivät saa sitä helposti tietoonsa (telnet yms). Liittyen suolaukseen, suola voitaisiin ehkä koostaa käyttäjän profiilin tiedoista (jotka siis eivät ole muutettavia, kuten userid, tai vastaavat autoincrement numero), lisäämään tietoaknnan murtamisen kustannuksia (kenties voitaisiin tallentaa tietokantaan vale salt tietue jota ei käytetä..). Tällöin tosin menetetään mainittu hyöty jos käyttäjä vaihtaa salasanaa. Sen lisäksi tällöisellä menetelmällä on tärkeää, ettei hyökkääjä saa haltuunsa palvelun lähdekoodia/binaaria, josta voidaan selvittää mitä käytetään salt-tietueena. Varmaan samalla voitaisiin olettaa, ettei hyökkääjä saa haltuunsa tietokantaakaan. Varmaan parempi ratkaisu olla soveltamatta itse ja käyttää suositeltvia menetelmiä.

Yllättävästi olin luulossa, että biometriikalla oltaisiin päästy parempiin tuloksiin. Tosin uudet uutiset, jossa iirisskanneria huijataan valokuvalla ja piilolinssillä kielivät, ettei ainakaan tämä tekniikka ole murtamaton. Myös huomio, että miten käyttäjäystävällinen biometriikka on tapauksissa, missä käyttäjä kokee muutoksia, esimerkiksi teloo itsensä sahalla ja sormenjälki on tällöin käyttökelvoton. Tällöin käyttäjän pääsynpalautus järjestelmä on tarpeen ja saattaa olla haavoittuva samoille menetelmillä, joille nykyiset järjestelmätkin ovat. Aukkoja voivat olla heikot policyt (ei tarpeelliset tarkistukset tai varmentamiseen käytetään julkista tietoa) tai itse käyttäjät. Häpeäkseni on tunnustettava, että oeln itsekkin sortunut käyttämään palautuskysymyksessä jäätävän helppoa kysymystä, joka itsessään paljastaa vastauksen. Puollustuksekseksi on mainittava, ettei näin tehty julkisen palvelun kohdalla, vaan järjestelmässä johon on pääsy vain paikallisesti. Melkein luulisi, että kohdennetussa hyökkäyksessä isketään juuri salasanan/tilin palautus mekanismia vastaan. Mitä tulee paperin tuloksiin, niin tunnustautumismekanismia on vaikea valita, ennen kuin voidaan määritellä hallittava/hyväksyttävä riski sen väärinkäytölle/hajoamiselle. Tällä tarkoitan, että jos palvelussa käsitellään arkaluontoista tietoa, kuten pankit tai sotilaskohteet, niin valittu mekanismi eroaa selkeästi esmerkiski musiikkipalvelun mekanismista. Yleensä kriittisissä palveluissa epämukavuus käyttäjän kannalta ja käyttöönoton vaikeus on hyväksyttävä kustannus, koska riskit ovat suuremmat.

## Luento 10

Ennen materiaaliin tutustumista ja videon katsomista, käsitykseni symbolisesta suorituksesta oli seuraavanlainen: muuttujat korvataan, noh symbolisella muuttujalla, ja tämä ohjelma sitten ajetaan erilaisilla muuttujien variaatioilla kunnes se joutuu tilaan joka keskeyttää ajon. Ajattelin, että symbolisen suorituksen toteuttava osa hyödyntää muistin muokkausta (käännetyillä -), tai lennosta muuttamista (tulkittavilla kielillä). Perusperiaatte oli, että lähdekoodin johdosta luodaan suorituspuut tai graafit ja ne ajetaan vuorotellen. Tässä voidaan kyseenalaistaa, että saavutetaan tämän tapaisella menettelyllä yhtään mitään, perinteiseen testaukseen verraten. Videossakin manittiin, että aikaisemmalla mallilla ei voida todistaa ohjelman käyttäytyvän oikein kaikilla syötteen arvoilla, jos testit suunnitellaan joidenkin rajoituksien mukaan. Rajoitukset voivat olla peräisin vaikka haarautuvien rakenteiden parametreiseta, kuten luennoilla on esitelty. Ongelma siis on että ollaan todistettu vain ohjelman toiminta näiden arvojen kanssa. 

Symbolisen tarkistuksen tai ratkomisen perusmekanismi vaatii lisää tutustumista. Yleiskuvaus on että SMT solver koostuu ns. teorioista ja SAT ratkaisijasta, joka yrittää ratkaista kaavalle (joka saa boolean arvoja/muuttujia) sen toteuttavat tekijät. Se voi myös ilmoittaa, ettei kaavalle ole ratkaisua tai ei päästy mihinkään ratkaisuun (kyse taitaa olla tässä jonkinlaisesta aikakatkaisusta, järkevää sillä ongelmavaruus on iso). SMT luo SAT:lle boolean-yhtelön, erottamalla muuttujat ja korvaamalla ne booleanarvoilla, joille SAT löytyää (tai ei) ratkaisun. SAT-solver voi palauttaa toteuttavan yhtelön, jonka teoria-solver testaa jollain metodilla (simplex, yms). Näiden välille voidaan luoda feedback-loop, jossa teoria-solver voi ohjeistaa SAT hakemaan uuden ratkaisun, jossa otetaan huomioon edellisen ratkaisun mahdottomuus (jos SAT:n ratkaisua ei voida ratkaista).

Ratkaistavien yhtelöiden luominen ohjelmakoodista, jota voidaan analysoida, tai ratkaista SMT:llä voidaan tehdä jokaiselle ohjelman haaralle yksitellen. Tällöin ohjelman suorituksesta syntyy puurakenne, missä on ohjelman suorituksen polkut. Testaaja ylläpitää tietoa jokaisen polun muuttujien ja rajoituksista. Tämä ei ole puhdasta symbolista tarkistusta. Miten minä sen ymmärsin, oli että etsitään ohjelmasta suurin osa poluista (tai kaikki, riippuen ohjelman monimutkaisuudesta ja vaaditusta testauksen kattavuudesta.) ja etsitään SMT- yms solvereilla sopivat syötteet, joilla ohjelma käy polut läpi. Tällä tavalla voisi ainakin luulla saavansa selville toimivatko kaikki ohjelman osat. Kuten luennolla mainittiinkin, niin saatetaan ohjelma ensin ajaa normaalista läpi, tarkoituksena kerätä ensimmäinen polku, josta muut etsitään solvereilla vaihtamalla haarakohdissa suoritusta (vaikka leluna x < y). Koska tästä voidaan rakentaa polkupuu, niin testaamattomien alueiden kanssa voidaan vaikka lähteä liikkeelle lähimmästä tunnetusta solmusta/haarasta. Huomioitava, että symbolic ja edellä mietitty ovat eriasioita. Symbolinen menetelmä toimii puhtaiden yhtelöiden kanssa (taisivat olla vielä boolen, eli (A and B) or C), jolloin ohjelman yhtelö esittää haarautuneet polut yhtenä yhtelönä. Taitaa menne liian vaikeksi ratkaista, tällöin hybridi metodi jossa käytetään polkuehtoja on parempi ratkaisu.

KLEE on suunnattu low level bugien löytämiseen, missä testit yleensä pyrkivät tunnistamaan onko ohjelmiston palauttama arvo/olio/yms oletetun muotoinen ja kelvollinen. KLEE:ssä syötteet ohjelmalle korvataan symboolisilla muuttujilla ja pidetään yllä suorituspolkuja, joihin siirrytään suorituksen haarautumiskohdista. Haarakohdassa selvitetään mitkä ovat haarautumisperusteen mahdolliset rajoitukset (ie. i < j) joiden perusteella luodaan testitapaukset (i < j ja i >= j). KLEE:n etuna on julkaisun tuloksie mukaan laaja testauksen kattavuus. Minusta mielenkiintoista on, että tarkistuksen luoma testi voidaan suorittaa jo käännetyilla ohjelmilla, helpottaen virheenkorjausta, koska ohjelmoija voi käyttää tätä testiä luomaan tilanteen missä virhe esiintyy. Yleensä erinäiset testaustyökalut ovat vaatineet lähdekoodin toimiakseen. Tällä näyttää olevan mielenkiintoisia mahdollisuuksia, etenkin jos tekniikka saadaan integroitua helposti käytettäväksi osaksi suositumpia kehitysympäristöjä. Vedetään edellinen maininta takaisin, KLEE vaatii että ohjelmat on käännetty bytecode:ksi LLVM-kääntäjällä (low level virtual machine). Paperissa manittiin, että KLEE mahdollistaa ympäristömuuttujien käytön tetauksessa. Esimerkkinä annettiin IO-vihe tai timeout. Tällähän on selvä vaikutus testauksen ulottumiseen, koska näin testi etenee virheiden takia poikkeutushaaroihin. 

## Luento 8

Luettavissa materiaaleissa oli top10 listaus internetpalveluiden tietoturvaongelmista vuodelta 2013. Kiinnostavaa on että julkaisuehdokaslista vuodelle 2017 sisältää lähes identtisen listan (saatavilla: https://github.com/OWASP/Top10/raw/master/2017/OWASP%20Top%2010%20-%202017%20RC1-English.pdf), eli muutokset neljän vuoden aikana eivät ole olleet merkittäviä. Sen lisäksi muutokset koskivat lähinnä sijoja 7 ja 10, vanha tuttu injektio pitää edelleen päässään kruunua. Uudet ehdokkaat olivat A7:riittämättömät hyökkäyksen puollustuskeinot ja A10:huonosti turvatut rajapinnat. A7 sisältää mm. lokin ja puolustuksen hyökkäyksiä vastaan, joka voi olla vaikka timeout tai fail2ban tyyppinen mekanismi, kenties myös websovelluspalomuurit voidaan lukea tähän kirjoon? Listassa manittiin usein rajapinta ja tiedon vuotaminen, joka tuo mieleen tapaukset missä automaattisesti etsimällä ja käsittelemällä julkista/avointa pilvipalvelun rajapintaa (tai skannaamalla git- yms tietokantaa) etsitään tietoa. Tapauksia on missä on löydetty ssh-avaimia githubista ja kokonaisia tietokantoja Glasier-palvelusta. Kenties näitä olisi voitu estää huomioimalla heti tiedostot/tiedot, joita ei saa olla avoimena ja säätämällä baikka lähetys tai buildskriptit sitä silmällä pitäen. A9: tunnettujen haavoittuvien komponettien käyttö, varmaan helpoin esimerkki löytyy wordpress-lisäosista, on ihmeellinen. Sitä kun luulisi, että korjaavan päivityksen olemassa olon voisi tunnistaa ohjelmallisesti tai tarkistuttaa, jos käytetyt komponetit ovat haavoittuvia mutta kuten listalla manittiin, niin erinäiset listauksen erot ja muut tekijät tekevät haav. komp. tunnistamisesta automaattisesti hankalaa. Eikö tähän ole olemassa jotain ratkasua, kuten kompontetti tai ohjelmisto inventaariota? Tosin ongelma on myös käyttäjän riesana, yhtenäistä mekanismia jolla voitaisiin hakea järjestelmän kaikkien ohjelmien uudet päivitykset tai edes tarkistaa niin olemassa olo on ... yllättävän haastavaa ja normikäyttäjälle kenties vaikeaakin. Pääasissa, jos sovellus ei tarkistuta päivityksiä, niin (poislukien, jotkin "hallinta/manageri" ohjelmat) uusien versioiden ja korkauksien etsiminen jää käyttäjälle (suurin riesa windows ympäristössä). Mitens sitten nää erilaiset app storet, jotka tekevät tuloaan myös Windows storen myötä? Taidetaan tässäkin törmätä samaan ongelmaan, joka on esillä injektioissa, eli vanhan koodin määrä ja levinnäisyys. Muutama huomio ja miete luennon aikana:

* MIME sniffing oli aika yllätys. Etenkin kun miettiin että selain pyrkii tulkkaamaan kaiken mikä vähänkin muistuttaa HTML:llää.. 
* Quirksmode - näyttää olevan aika <del>helv</del> vaikeaa tehdä sivuja, jotka toimivat kaikilla selaimilla ja alustoilla. Ei ole ihme, jos mobiilisivustojen kanssa jotkin toimijat (mm. reddit) pakottavat joka sivullaan jotain erilistä lukusovellusta.
* web app koostuu useiden toimijoiden osista -> yksikin haavoittuvuus voi olla riittävä. Hyökkäyspinta ala on laaja web app:lla, puhumattakaan itse selaimen h.-pintaalasta. 
* Mal.Ad eli hyökkäykset käyttäen mainostoimijoita lävittäjinä. Ei mitään uutta mutta vapauttaa hyökkääjän levittämään hyökkäyksen ilman että tarvitaan huijata käyttäjää heidän sivuilleen tai tarvetta kaapata jonkin muun palvelin.
* Same-origin: vain samasta lähteestä haetut resurssit voivat vaikuttaa keskenään. On tarve kuitenkin sallia tämä jossain tapauksissa. Lähde määriteltiin protokolan, isäntänimen (domain?) ja porttinumeron (palvelimen) yhdistelmänä. Oletuksena näyttää olevan, että alidomainit käsitellään erillisenä mutta js-koodissa voidaan löysätä politiikkaa koskemaan ylemmän asteen domainia. Eli tässä tapauksessa voidaan luoda poikkeus ja heikentää same-origin käytäntöä. Heikentäminen on mahdollista myös iframien kanssa, eli kehittäjien on oltava tietoisia tästä ja huomioita heikentyneet turvaominaisuudet.
* Keksit ovat hyvä kohde suojata, koska niissä on käyttäjäkohtaista dataa, kuten istuntotunnisteita.
* AJAX-pyynnöt vain saman originin sisällä, ellei CROS ole sallittu. Törmäsin tähän, kun yritin tehdä omaan käyttöön useasta sijannista tietoa hakevaa sivua (dataa ei voinut hakea julkisen rajapinnan kautta). Muistaakseni tässä oli jotain hienoa millä voitiin huijata tätä systeemiä. JSONP taisi olla keino jakaa tietoa eri originien välillä, ennen kuin CORS saatiin käyttöön.
* Jossain taidettiin mainita syötteiden filteröinti tai validointi selaimessa (tai sitten ei), kuitenkin minusta se liittyy aiheeseen ja siihen ei kannata missään nimessä luottaa, koska hyökkääjä halllitsee täysin selainta. Sama myös rajapintojen kanssa, SQLi on kyllä tuttua tietokantojen kanssa mutta muistetaanko sama tehdä myös rajapintojen kanssa? 
* Verkko-osoitteiden kanssa (koska origin-järjestelmä perustuu nimiin, joten dns pohjaiset hyökkäykset voivat olla pahoja) pitää asettaa nimipalvelimet siten, ettei ulkosia nimia ratkaista sisäisiin osoitteisiin. Toinen tapa on dns pinning, missä ensimmäinen nimipalvelupyyntö joka on validi pidetään voimassa vaikka sen jälkeen tehtäisiin uusia pyyntöjä.
* Clickjacking hyökkäyksien kanssa ei taida olla paljoa suojaa, joita käyttäjä voi realistisesti käyttää. Noscript on yksi mutta sekin on saatavilla vain firefox-selaimelle ja eikä sitä ole suunnattu persukäyttäjälle.

## Luento 9

Luettavissa materiaaleissa keskityttiin Cross Site Request Forgery (CSRF) mekanismiin ja sen vastatoimiin Django kehitysympäristössä. CSRF on hyökkäys, missä uhrin selain huijataan suorittamaan toiminto toisella sivulla, usein linkin tai lomakkeen avulla. Uhrin selain siis huijtaan luomaan väärennettyjä pyyntöjä kohdesivun ulkopuolelta (tällöin selain lähettää pyynnön ohessa kohdesivun keksit -> kohdesivu uskoo uhrin tehneen pyynnön tarkoituksella). Vastakeinona voidaan käytetää jotain mekanismia, joka varmentaa että saapuva pyyntö on tehty kohdesivun lomakkeesta tai muusta resurssista, tarkistamalla jonkin luodun tokenin arvon/olemassa olon. Idea on että hyökkääjällä ei ole tietoa tästä arvosta ja sen puuttumien johtaa pyynnön hylkäämiseen. Tarkemmin Djangossa tarkasteltiin middlewaren "CsrfViewMiddleware" käyttöä ja määrittelyä. Se perustuu muuttuvaan arvoon, joka saa uuden arvon käyttäjän kirjautumisen yhteydessä ja se tallennetaan CSRF keksiin. Sen lisäksi lomakkeissa on piilotettu kenttä 'csrfmiddlewaretoken', jonka lisäys hoidetaan templaten {% csrf_token %}-tagilla. Tag tulee liittää kaikkiin sisääntuleviin form POST-pyyntöihin. Vastaavasti AJAX-pyynnöillä voidaan käyttää X-CSRFToken-otsikkoa. Minusta on hyvä asia, että kehitysympäristöihin kuten Djangoon tai vastaavaan sisällytetään sisäänrakennettuja turvallisuusmekanismeja, jotka laskevat kynnystä käyttää tekniikoita (kuten csrf). Koska on mahdollista unohtaa otta käyttöön nämä mekanismit, niin ympäristön tulisi automaattisesti käyttää niitä, ellei erikseen estetä, tavallaan siis eräänlainen whitelist niille näkymille missä suojat eivät ole päällä. Nämä näkymät tulisivat olla vain staattista tai sisältöä, joka ei ole lähtöisin ulkopuolelta (eli epäluotettavasta lähteestä).

Aina jaksaa yllättää miten yleisiä verkkoselaimet ovat, eli kuinka moni normaalilta tuntuva sovellus pitää sisällään jonkinlaisen verkkoselaimen tai sen html-moottorin. Esimerkiksi Valven Steam-asiakasohjelmisto käyttää sisäistä chrome pohjaista moottoria näyttämään sisältöä. Tämä on tärkeää, koska heillä kävi perinteiset. Käyttäjät voivat lähettää oppaita HTML-tyylitettynä, jotka tallennetaan tietokantaan. Aukko löytyi oppaan otsikko-kentästä, joka mahdollisti täysin perinteisen leluesimerkin <script></script> käytön, koska kentän sisältöä ei käsitelty ja tallennettiin suoraan sellaisenaan. Vaikka title-kenttä oli rajoitettuna 128-merkkiin, niin suodatuksen puuttuminen mahdollisti eval(#larger_text.text()) funktion käytön. Tässä olisi voitu käyttää useampaa luennoillakin esiteltyä keinoa estotoimenpiteenä. Koska aukko löytyi galleria/käyttäjän opas-osion otsikosta, niin vähemmän toiminnallisen kielen pakottaminen olisi voinut olla paras vaihtoehto. 

Content Security Policy (CSP) näyttää olevan hyvin tehokas paikkaus XSS-hyväksikäyttöihin, jos samalla hyödynnetään erillisia domainnimiä scripteille ja muulle sisällölle. Erottelun tarkoitus on rajoittaa XSS-haavoittuvuuden laajuutta ja välttää keksien yms. joutumisen hyökkääjän javascriptin käsiin. Sanitaation lisäksi kyllä vähemmän kuvaavan kielen käyttö saattaa olla hyvä ratkaisu. Onkohan enään mahdollista saada selain toimittajat noudattamaan määrityksiä? Luennoitsija tarjosi SQLi:hin ratkaisuksi sanitointia ja tiettyjen merkkien neutralisointia, mikä ei ole näinä päivänä suositeltu ratkaisu (vai onko edes ratkaisu ylipäätään), kun prepared statement on tuettuna yleisimmissä tietokantarajapinnoissa/ajureissa. 

Jotenkin vaikea keksiä miten esitellyt sijainnin nuuskinta hyökkäykset olisivat hyödyllisiä, paitsi erittäin kohdennetuissa hyökkäyksissä. Resurssien pääsy kontrolli on vähän paha paikka polkaista itse pystyyn. Pitäisi olla osa suunnitelmaa alusta asti, eikä mikään viimehetken lisäys, jotta saataisiin kunnolla kontrolloitua. Tässä alleviivaisin ympäristön roolia, jos moisessa on tuki tälläiselle. Esimerksi se Django on yksi. Kysymys onkin, että mitä suojataan pääsykontrollilla ja miten se kontrolli toteutetaan käytännössä. Rajapinnan kanssa varmaan ensin salaus ja sitten joko token (tai poletti) tai uid/pw combo? Tässä joutaa kyllä pohtimaan mikä on sopiva suojauksen/tarksituksen määrä. Tarvitaanko suojausta jokaisen funktion kanssa vai riittääkö kerta tarkistus näkymän kohdalla? Kenties elintärkeä toiminnon pitäisi piilottaa certifikaatin taakse, toisin sanoen vain käyttäjä jolla on hyväksytty/myönnetty cert voi suorittaa resurssin. 

## Luento 18

Selaimet kehittyvät, niihin tuodaan uusia omianaisuuksia osana kilpailua markkinoista ja tämän yhteydessä ei huomata tai haluta miettiä miten lisäykset vaikuttavat yksityisyyteen. Private browsing määritellään paikallisesti ajan hetkellä T ei voida kerätä tietoa mitä käyttäjä on tehnyt ajanhetkellä ennen T:tä. Plausible deniability taasen on ominaisuus tai tapahtumien tila, joka mukaan voidaan kieltää jokin seikka, koska todisteet eivät ole riittävän vahvat. Mitä private browsing voi vuotaa: *cookies,dom-storage *selaimen välimuisti *selain historia *asetukset, asiakas-sertifikaatit, kirjanmerkit, ladatut tiedostot, tallennetut salasanat? *asennetut lisäosat ja laajennukset.

Verkkoliikenne voi myös vuotaa käyttäjän aktiviitin:  dns-nimipyynnöt, verkkolokit. Vaikka käytetään VPN-palvelua tai välityspalvelinta, niin ketjun jossain päässä on tiedossa mistä pyyntö on saapunut. Parasta yksityisyydelle on olla tallentamatta näitä tietoja lokiin. Muistijäännökset -> sivutustiedostossa voi olla jäänteitä muistista. Pitäiskö yksityinen sessio ajaa omassa prosessissa? Alkaa näyttämään, että jos halutaan tehdä secure deallocate, niin tarvitaan muutoksia käyttöjärjestelmään ja selaimeen. Tai data salataan (xor ?) ennenkuin se kirjoitetaan pysyvään mediaan (hdd, ssd, yms) ja vain salausavain nollataan. Tosin tässäkin tulee vastaan, että mitenkä tämä salausavain sitten tallennetaan Selain fingerprinting, yllätävän paljon asiaa vuotaa. Voidaanko sen päälle vielä heittää käytäjän hiiren liikkeen tallentaminen ja analysoiminen? Tässä voi olla paha paikka koneoppimisen kanssa, joka on hyvä löytämään malleja ja toistuvia tapauksia, sekä yhteyksiä.

Takeet: 
* VM yksityinen sessio VM-instanssissa joka tuhotaan session lopetuksen yhteydessä. Mahdollisisti takaa että käyttöjärjestelmään ei jää jälkiä. Ei vaadi muutoksia softaan, mutta tehohävikki on todellinen ja virtuaalikoneen käynnistys on hidasta. 
* Käyttöjärjestelmän tasolle suoritettava eristys/virtualisointi, käyttöjärjestelmä pitää yllä tietoa mitä yksityisessä sessiossa tehtiin ja poistaa ne session jälkeen. Vaikeampi toteuttaa oikein, mutta on suorituskyvyltään kevyempi. 

Virtuaalikoneita saatetaan tunnistaa, tai hypervisor on seurattava/tunnistettava. TCP-sormenjälki, kuten miten TCP paketti on luotu, kenttien oletusarvot. Esimerkki nmap ja muut kokoelmat tunnetuista käyttöjärjestelmistä. Voidaanko tätä estää? Tuskin ja onhan käyttöjärjestelmien tunnistamiselle myös käytännön hyödyt. Muita tunnistustekijöitä voivat olla HTTP-otsikon kentät, kuten user-agent. Jos halutaan vaikeuttaa tunnistusta, niin jokaisen tulisis siis käyttää samaa käyttöjärjestelmää. Käytännössä vaikea toteuttaa, tuleekohan Tor-projektilta seuraavaksi TorOS virtuaalipakettina?

Selaimen tila voi muuttua 1) kun palvelin tekee jotain (keksi, selain historia, välimuisti). Ei käyttäjän toimia, tarkoitus että muutokset tuhotaan kun session on ohi. 2) palvelin tekee jotain ja se vaatii käyttäjän toimia (salanan talennus, yms) joka säilyy session ulkopuolelle. 3) Käyttäjän aloittamat toiminnut yleensä säilyvät session yli, kuten tiedostojen lataukset tai kirjanmerkit. 4) muutokset session ulkopuolella. 
Mielenkiintoisisti windowsin index.dat sisälsi joku vuosi sitten selaushistoriaa (+tiedostopolkuja) yllättävän pitkältä ajalta. En ole varma mitä kaikkea siihen tallennetaan. Joka myös tukee esitettyä mielipidettä, että selainten yksityinen tila ei anna niin vahvaa suojaa, kuin voidaan luulla. Kuten luennoillakin taidiettiin mainita, niin suoja on lähinnä vähemmän teknistä vastustajaa vastaan hyväksyttävämmällä tasolla. 

Totuus taitaa olla surullisesti, että käyttäjät eivät ole kiinnostuneita tai priorisoivat muita tekijöitä yksityisyyden yli. Tällöin mekanismit, jotka ovat teknisiä tai vaativat käyttäjän toimia (VM, yms) eivät saa tarpeeksi laajaa käyttäjäkuntaa että niiden ylläpito tai kehitys on kannattavaa. Tutkimus- tai muun pohjaiset ratkaisut saattavat paikata tämän aukon mutta näiden käyttäjät ovat todennäköisesti teknisesti kykeneviä käyttämään erillistä virtuaaliinstanssia. Toinen syy voi olla chromen osalta, että Googlen yritysmalli nojaa käyttäjien seurantaan ja käyttäymiseen persutuvaan kustomointiin (microsfot ja edge taitavat suunnata samalle polulle).
